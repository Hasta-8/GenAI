{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "8EwFGgAvE2zz"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Notebook Summary"
      ],
      "metadata": {
        "id": "8EwFGgAvE2zz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3a41a1f"
      },
      "source": [
        "This notebook demonstrates the process of building a question-answering system using LangChain and Pinecone, leveraging the content of PDF documents. The key techniques used are:\n",
        "\n",
        "*   **Loading Documents:** Using `PyPDFDirectoryLoader` to load text content from PDF files in a specified directory.\n",
        "*   **Text Splitting:** Employing `RecursiveCharacterTextSplitter` to break down the extracted text into smaller, manageable chunks for processing.\n",
        "*   **Generating Embeddings:** Utilizing `OpenAIEmbeddings` to create vector representations of the text chunks. These embeddings capture the semantic meaning of the text.\n",
        "*   **Initializing Pinecone:** Setting up a connection to the Pinecone vector database using your API key.\n",
        "*   **Creating/Loading a Vector Store:** Using `PineconeVectorStore` to either create a new index in Pinecone with the generated embeddings or load an existing one. This vector store allows for efficient similarity search.\n",
        "*   **Similarity Search:** Performing similarity searches on the vector store to find document chunks most relevant to a given query.\n",
        "*   **Setting up a RetrievalQA Chain:** Configuring a `RetrievalQA` chain with a language model (`OpenAI`) and the vector store retriever. The `chain_type=\"stuff\"` is used to pass the retrieved document chunks as context to the language model.\n",
        "*   **Question Answering:** Using the `RetrievalQA` chain to answer questions based on the context provided by the retrieved document chunks.\n",
        "*   **Interactive Chat Loop:** Implementing a simple command-line interface for interactive question answering with the built system."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Loading required packages**"
      ],
      "metadata": {
        "id": "frRRCC8oES5U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dde7nS5EIZex"
      },
      "outputs": [],
      "source": [
        "!pip install -q langchain_pinecone langchain_openai langchain_community pinecone openai tiktoken --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Load the PDF Files**"
      ],
      "metadata": {
        "id": "MiaJVHxLe2F4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir pdfs"
      ],
      "metadata": {
        "id": "FVvwxXice4aA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Extract the Text from the PDFs**"
      ],
      "metadata": {
        "id": "l2sEDBz7fW-2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a97fef9a"
      },
      "source": [
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "\n",
        "loader = PyPDFDirectoryLoader(\"pdfs/\")\n",
        "text_data = loader.load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qpzL9TbmfnJ7",
        "outputId": "e526a3df-ecde-4a7d-d34c-bb3600feb9bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(metadata={'producer': 'macOS Version 12.6.1 (Build 21G217) Quartz PDFContext', 'creator': 'Microsoft Word', 'creationdate': '2023-03-17T18:08:34+00:00', 'author': 'NVIDIA', 'keywords': 'Large Language Models, What is a large language model, llm, how do large language models work', 'moddate': '2023-03-17T11:30:56-07:00', 'subject': 'Large Language Models', 'title': 'A Beginner’s Guide to Large Language Models', 'source': 'pdfs/nvidia_llm.pdf', 'total_pages': 25, 'page': 0, 'page_label': '1'}, page_content='A Beginner’s Guide to \\nLarge Language Models\\nPart 1\\nContributors:\\nAnnamalai Chockalingam\\nAnkur Patel\\nShashank Verma\\nTiffany Yeung'), Document(metadata={'producer': 'macOS Version 12.6.1 (Build 21G217) Quartz PDFContext', 'creator': 'Microsoft Word', 'creationdate': '2023-03-17T18:08:34+00:00', 'author': 'NVIDIA', 'keywords': 'Large Language Models, What is a large language model, llm, how do large language models work', 'moddate': '2023-03-17T11:30:56-07:00', 'subject': 'Large Language Models', 'title': 'A Beginner’s Guide to Large Language Models', 'source': 'pdfs/nvidia_llm.pdf', 'total_pages': 25, 'page': 1, 'page_label': '2'}, page_content='A Beginner’s Guide to Large Language Models  2 \\nTable of Contents \\nPreface ....................................................................................................................................................... 3 \\nGlossary ...................................................................................................................................................... 5 \\nIntroduction to LLMs.................................................................................................................................. 8 \\nWhat Are Large Language Models (LLMs)? .......................................................................................... 8 \\nFoundation Language Models vs. Fine-Tuned Language Models ...................................................... 11 \\nEvolution of Large Language Models ................................................................................................. 11 \\nNeural Networks ............................................................................................................................. 12 \\nTransformers .................................................................................................................................. 14 \\nHow Enterprises Can Benefit From Using Large Language Models......................................................... 20 \\nChallenges of Large Language Models ............................................................................................... 21 \\nWays to Build LLMs ............................................................................................................................. 21 \\nHow to Evaluate LLMs ........................................................................................................................ 22 \\nNotable Companies in the LLM Field .................................................................................................. 23 \\nPopular Startup-developed LLM Apps ................................................................................................ 23'), Document(metadata={'producer': 'macOS Version 12.6.1 (Build 21G217) Quartz PDFContext', 'creator': 'Microsoft Word', 'creationdate': '2023-03-17T18:08:34+00:00', 'author': 'NVIDIA', 'keywords': 'Large Language Models, What is a large language model, llm, how do large language models work', 'moddate': '2023-03-17T11:30:56-07:00', 'subject': 'Large Language Models', 'title': 'A Beginner’s Guide to Large Language Models', 'source': 'pdfs/nvidia_llm.pdf', 'total_pages': 25, 'page': 2, 'page_label': '3'}, page_content='A Beginner’s Guide to Large Language Models 3 \\n \\nPreface \\nLanguage has been integral to human society for thousands of years. A \\nlong-prevailing theory, laryngeal descent theory or LDT, suggests that speech and, thus, language, \\nmay have evolved about 200,000 or 300,000 years ago, while newer research shows it could’ve \\nhappened even sooner. \\nRegardless of when it first appeared, language remains the cornerstone of human communication. It \\nhas taken on an even greater role in today’s digital age, where an unprecedented portion of the \\npopulation can communicate via both text and speech across the globe. \\nThis is underscored by the fact that 347.3 billion email messages are sent and received worldwide \\nevery day, and that five billion people – or over 63% of the entire world population – send and receive \\ntext messages.  \\nLanguage has therefore become a vast trove of information that can help enterprises extract valuable \\ninsights, identify trends, and make informed decisions. As an example, enterprises can analyze texts \\nlike customer reviews to identify their products’ best-selling features and fine-tune their future \\nproduct development.  \\nSimilarly, language production – as opposed to language analysis – is also becoming an increasingly \\nimportant tool for enterprises. Creating blog posts, for example, can help enterprises raise brand \\nawareness to a previously unheard-of extent, while composing emails can help them attract new \\nstakeholders or partners at an unmatched speed.  \\nHowever, both language analysis and production are time-consuming processes that can distract \\nemployees and decision-makers from more important tasks. For instance, leaders often need to sift \\nthrough vast amounts of text in order to make informed decisions instead of making them based on \\nextracted key information.  \\nEnterprises can minimize these and other problems, such as the risk of human error, by employing \\nlarge language models (LLMs) for language-related tasks. LLMs can help enterprises accelerate and \\nlargely automate their efforts related to both language production and analysis, saving valuable time \\nand resources while improving accuracy and efficiency.  \\nUnlike previous solutions, such as rule-based systems, LLMs are incredibly versatile and can be easily \\nadapted to a wide range of language-related tasks, like generating content or summarizing legal \\ndocumentation.'), Document(metadata={'producer': 'macOS Version 12.6.1 (Build 21G217) Quartz PDFContext', 'creator': 'Microsoft Word', 'creationdate': '2023-03-17T18:08:34+00:00', 'author': 'NVIDIA', 'keywords': 'Large Language Models, What is a large language model, llm, how do large language models work', 'moddate': '2023-03-17T11:30:56-07:00', 'subject': 'Large Language Models', 'title': 'A Beginner’s Guide to Large Language Models', 'source': 'pdfs/nvidia_llm.pdf', 'total_pages': 25, 'page': 3, 'page_label': '4'}, page_content='A Beginner’s Guide to Large Language Models 4 \\n \\nThe goal of this book is to help enterprises understand what makes LLMs so groundbreaking \\ncompared to previous solutions and how they can benefit from adopting or developing them. It also \\naims to help enterprises get a head start by outlining the most crucial steps to LLM development, \\ntraining, and deployment.  \\nTo achieve these goals, the book is divided into three parts: \\n> Part 1 defines LLMs and outlines the technological and methodological advancements over the \\nyears that made them possible. It also tackles more practical topics, such as how enterprises can \\ndevelop their own LLMs and the most notable companies in the LLM field. This should help \\nenterprises understand how adopting LLMs can unlock cutting-edge possibilities and revolutionize \\ntheir operations.  \\n> Part 2 discusses five major use cases of LLMs within enterprises, including content generation, \\nsummarization, and chatbot support. Each use case is exemplified with real-life apps and case \\nstudies, so as to show how LLMs can solve real problems and help enterprises achieve specific \\nobjectives. \\n> Part 3 is a practical guide for enterprises that want to build, train, and deploy their own LLMs. It \\nprovides an overview of necessary pre-requirements and possible trade-offs with different \\ndevelopment and deployment methods. ML engineers and data scientists can use this as a \\nreference throughout their LLM development processes. \\nHopefully, this will inspire enterprises that have not yet adopted or developed their own LLMs to do \\nso soon in order to gain a competitive advantage and offer new SOTA services or products. The most \\nbenefits will be, as usual, reserved for early adopters or truly visionary innovators.'), Document(metadata={'producer': 'macOS Version 12.6.1 (Build 21G217) Quartz PDFContext', 'creator': 'Microsoft Word', 'creationdate': '2023-03-17T18:08:34+00:00', 'author': 'NVIDIA', 'keywords': 'Large Language Models, What is a large language model, llm, how do large language models work', 'moddate': '2023-03-17T11:30:56-07:00', 'subject': 'Large Language Models', 'title': 'A Beginner’s Guide to Large Language Models', 'source': 'pdfs/nvidia_llm.pdf', 'total_pages': 25, 'page': 4, 'page_label': '5'}, page_content='A Beginner’s Guide to Large Language Models 5 \\n \\nGlossary  \\nTerms Description \\nDeep learning systems Systems that rely on neural networks with many hidden layers to \\nlearn complex patterns. \\nGenerative AI AI programs that can generate new content, like text, images, \\nand audio, rather than just analyze it. \\nLarge language models (LLMs) Language models that recognize, summarize, translate, predict, \\nand generate text and other content. They’re called large \\nbecause they are  trained on large amounts of data and have \\nmany parameters, with popular LLMs reaching hundreds of \\nbillions of parameters. \\nNatural language processing (NLP) The ability of a computer program to understand and generate \\ntext in natural language. \\nLong short-term memory neural network (LSTM) A special type of RNNs with more complex cell blocks that allow \\nit to retain more past inputs.  \\nNatural language generation (NLG) A part of NLP that refers to the ability of a computer program to \\ngenerate human-like text. \\nNatural language understanding (NLU) A part of NLP that refers to the ability of a computer program to \\nunderstand human-like text. \\nNeural network (NN) A machine learning algorithm in which the parameters are \\norganized into consecutive layers. The learning process of NNs is \\ninspired by the human brain. Much like humans, NNs “learn” \\nimportant features via representation learning and require less \\nhuman involvement than most other approaches to machine \\nlearning. \\nPerception AI AI programs that can process and analyze but not generate data, \\nmainly developed before 2020. \\nRecurrent neural network (RNN)  Neural network that processes data sequentially and can \\nmemorize past inputs.'), Document(metadata={'producer': 'macOS Version 12.6.1 (Build 21G217) Quartz PDFContext', 'creator': 'Microsoft Word', 'creationdate': '2023-03-17T18:08:34+00:00', 'author': 'NVIDIA', 'keywords': 'Large Language Models, What is a large language model, llm, how do large language models work', 'moddate': '2023-03-17T11:30:56-07:00', 'subject': 'Large Language Models', 'title': 'A Beginner’s Guide to Large Language Models', 'source': 'pdfs/nvidia_llm.pdf', 'total_pages': 25, 'page': 5, 'page_label': '6'}, page_content='A Beginner’s Guide to Large Language Models 6 \\n \\nTerms Description \\nRule-based system A system that relies on human-crafted rules to process data.  \\nTraditional machine learning Traditional machine learning uses a statistical approach, drawing \\nprobability distributions of words or other tokens based on a \\nlarge annotated corpus. It relies less on rules and more on data. \\nTransformer A type of neural network architecture designed to process \\nsequential data non-sequentially. \\nStructured data Data that is quantitative in nature, such as phone numbers, and \\ncan be easily standardized and adjusted to a pre-defined format \\nthat ML algorithms can quickly process.   \\nUnstructured data Data that is qualitative in nature, such as customer reviews, and \\ndifficult to standardize. Such data is stored in its native formats, \\nlike PDF files, before use. \\nFine-tuning A transfer learning method used to improve model performance \\non selected downstream tasks or datasets. It’s used when the \\ntarget task is similar to the pre-training task and involves copying \\nthe weights of a PLM and tuning them on desired tasks or data. \\nCustomization A method of improving model performance by modifying only \\none or a few selected parameters of a PLM instead of updating \\nthe entire model. It involves using parameter-efficient \\ntechniques (PEFT). \\nParameter-efficient techniques (PEFT) Techniques like prompt learning, LoRa, and adapter tuning \\nwhich allow researchers to customize PLMs for downstream \\ntasks or datasets whil preserving and leveraging existing \\nknowledge of PLMs. These techniques are used during model \\ncustomization and allow for quicker training and often more \\naccurate predictions. \\nPrompt learning An umbrella term for two PEFT techniques, prompt tuning and \\np-tuning, which help customize models by inserting virtual token \\nembeddings among discrete or real token embeddings. \\nAdapter tuning A PEFT technique that involves adding lightweight feed-forward \\nlayers, called adapters, between existing PLM layers and \\nupdating only their weights during customization while keeping \\nthe original PLM weights frozen. \\nOpen-domain question answering Answering questions from a variety of different domains, like \\nlegal, medical, and financial, instead of just one domain.  \\nExtractive question answering Answering questions by extracting the answers from existing \\ntexts or databases.'), Document(metadata={'producer': 'macOS Version 12.6.1 (Build 21G217) Quartz PDFContext', 'creator': 'Microsoft Word', 'creationdate': '2023-03-17T18:08:34+00:00', 'author': 'NVIDIA', 'keywords': 'Large Language Models, What is a large language model, llm, how do large language models work', 'moddate': '2023-03-17T11:30:56-07:00', 'subject': 'Large Language Models', 'title': 'A Beginner’s Guide to Large Language Models', 'source': 'pdfs/nvidia_llm.pdf', 'total_pages': 25, 'page': 6, 'page_label': '7'}, page_content='A Beginner’s Guide to Large Language Models 7 \\n \\nTerms Description \\nThroughput A measure of model efficiency and speed. It refers to the \\namount of data or the number of predictions that a model can \\nprocess or generate within a pre-defined timeframe.  \\nLatency The amount of time a model needs to process input and \\ngenerate output.   \\nData Readiness The suitability of data for use in training, based on factors such \\nas data quantity, structure, and quality.'), Document(metadata={'producer': 'macOS Version 12.6.1 (Build 21G217) Quartz PDFContext', 'creator': 'Microsoft Word', 'creationdate': '2023-03-17T18:08:34+00:00', 'author': 'NVIDIA', 'keywords': 'Large Language Models, What is a large language model, llm, how do large language models work', 'moddate': '2023-03-17T11:30:56-07:00', 'subject': 'Large Language Models', 'title': 'A Beginner’s Guide to Large Language Models', 'source': 'pdfs/nvidia_llm.pdf', 'total_pages': 25, 'page': 7, 'page_label': '8'}, page_content='A Beginner’s Guide to Large Language Models 8 \\n \\nIntroduction to LLMs \\nA large language model is a type of artificial intelligence (AI) system \\nthat is capable of generating human-like text based on the patterns \\nand relationships it learns from vast amounts of data. Large language models \\nuse a machine learning technique called deep learning to analyze and process large sets of data, such \\nas books, articles, and web pages. \\nLarge language models unlocked numerous unprecedented possibilities in the field of NLP and AI. This \\nwas most notably demonstrated by the release of OpenAI’s GPT-3 in 2020, the then-largest language \\nmodel ever developed.  \\nThese models are designed to understand the context and meaning of text and can generate text that \\nis grammatically correct and semantically relevant. They can be trained on a wide range of tasks, \\nincluding language translation, summarization, question answering, and text completion. \\nGPT-3 made it evident that large-scale models can accurately perform a wide – and previously \\nunheard-of – range of NLP tasks, from text summarization to text generation. It also showed that \\nLLMs could generate outputs that are nearly indistinguishable from human-created text, all while \\nlearning on their own with minimal human intervention.  \\nThis presented an enormous improvement from earlier, mainly rule-based models that could neither \\nlearn on their own nor successfully solve tasks they weren’t trained on. It is no surprise, then, that \\nmany other enterprises and startups soon started developing their own LLMs or adopting existing \\nLLMs in order to accelerate their operations, reduce expenses, and streamline workflows. \\nPart 1 is intended to provide a solid introduction and foundation for any enterprise that is considering \\nbuilding or adopting its own LLM.  \\nWhat Are Large Language Models (LLMs)?  \\nLarge language models (LLMs) are deep learning algorithms that can recognize, extract, summarize, \\npredict, and generate text based on knowledge gained during training on very large datasets.  \\nThey’re also a subset of a more general technology called language models. All language models have \\none thing in common: they can process and generate text that sounds like natural language. This is \\nknown as performing tasks related to natural language processing (NLP).'), Document(metadata={'producer': 'macOS Version 12.6.1 (Build 21G217) Quartz PDFContext', 'creator': 'Microsoft Word', 'creationdate': '2023-03-17T18:08:34+00:00', 'author': 'NVIDIA', 'keywords': 'Large Language Models, What is a large language model, llm, how do large language models work', 'moddate': '2023-03-17T11:30:56-07:00', 'subject': 'Large Language Models', 'title': 'A Beginner’s Guide to Large Language Models', 'source': 'pdfs/nvidia_llm.pdf', 'total_pages': 25, 'page': 8, 'page_label': '9'}, page_content='A Beginner’s Guide to Large Language Models 9 \\n \\nAlthough all language models can perform NLP tasks, they differ in other characteristics, such as their \\nsize. Unlike other models, LLMs are considered large in size because of two reasons:  \\n1. They’re trained using large amounts of data. \\n2. They comprise a huge number of learnable parameters (i.e., representations of the underlying \\nstructure of training data that help models perform tasks on new or never-before-seen data). \\nTable 1 showcases two large language models, MT-NLG and GPT-3 Davinci, to help clarify what’s \\nconsidered large by contemporary standards.  \\nTable 1. Comparison of MT-NLG and GPT-3 \\nLarge Language Model Number of \\nparameters \\nNumber of tokens in \\nthe training data \\nNVIDIA Model:  Megatron-Turing Natural Language \\nGeneration Model (MT-NLG) \\n530 billion 270 billion \\nOpenAI Model:  GPT-3 Davinci Model 175 billion 499 billion \\n \\nSince the quality of a model heavily depends on the model size and the size of training data, larger \\nlanguage models typically generate more accurate and sophisticated responses than their smaller \\ncounterparts.'), Document(metadata={'producer': 'macOS Version 12.6.1 (Build 21G217) Quartz PDFContext', 'creator': 'Microsoft Word', 'creationdate': '2023-03-17T18:08:34+00:00', 'author': 'NVIDIA', 'keywords': 'Large Language Models, What is a large language model, llm, how do large language models work', 'moddate': '2023-03-17T11:30:56-07:00', 'subject': 'Large Language Models', 'title': 'A Beginner’s Guide to Large Language Models', 'source': 'pdfs/nvidia_llm.pdf', 'total_pages': 25, 'page': 9, 'page_label': '10'}, page_content='A Beginner’s Guide to Large Language Models 10 \\n \\nFigure 1. Answer Generated by GPT-3. \\n \\n \\nHowever, the performance of large language models doesn’t just depend on the model size or data \\nquantity. Quality of the data matters, too.  \\nFor example, LLMs trained on peer-reviewed research papers or published novels will usually perform \\nbetter than LLMs trained on social media posts, blog comments, or other unreviewed content. Low-\\nquality data like user-generated content may lead to all sorts of problems, such as models picking up \\nslang, learning incorrect spellings of words, and so on. \\nIn addition, models need very diverse data in order to perform various NLP tasks. However, if the \\nmodel is intended to be especially good at solving a particular set of tasks, then fine-tune it using a \\nmore relevant and narrower dataset. By doing so a foundation language model is transformed — from \\none that’s good at performing various NLP tasks across a broad set of domains – into a fine-tuned \\nmodel that specializes in performing tasks in a narrowly scoped domain.'), Document(metadata={'producer': 'macOS Version 12.6.1 (Build 21G217) Quartz PDFContext', 'creator': 'Microsoft Word', 'creationdate': '2023-03-17T18:08:34+00:00', 'author': 'NVIDIA', 'keywords': 'Large Language Models, What is a large language model, llm, how do large language models work', 'moddate': '2023-03-17T11:30:56-07:00', 'subject': 'Large Language Models', 'title': 'A Beginner’s Guide to Large Language Models', 'source': 'pdfs/nvidia_llm.pdf', 'total_pages': 25, 'page': 10, 'page_label': '11'}, page_content='A Beginner’s Guide to Large Language Models 11 \\n \\nFoundation Language Models vs. Fine-Tuned \\nLanguage Models \\nFoundation language models, such as the aforementioned MT-NLG and GPT-3, are what is usually \\nreferred to when discussing LLMs. They’re trained on vast amounts of data and can perform a wide \\nvariety of NLP tasks, from answering questions and generating book summaries to completing and \\ntranslating sentences.  \\nThanks to their size, foundation models can perform well even when they have little domain-specific \\ndata at their disposal. They have good general performance across tasks but may not excel at \\nperforming any one specific task.  \\nFine-tuned language models, on the other hand, are large language models derived from foundation \\nLLMs. They’re customized for specific use cases or domains and, thus, become better at performing \\nmore specialized tasks.  \\nApart from the fact that fine-tuned models can perform specific tasks better than foundation models, \\ntheir biggest strength is that they are  lighter and, generally, easier to train. But how does one actually \\nfine-tune a foundation model for specific objectives? \\nCurrently, the most popular method is customizing a model using parameter-efficient customization \\ntechniques, such as p-tuning, prompt tuning, adapters, and so on. Customization is far less time-\\nconsuming and expensive than fine-tuning the entire model, although it may lead to somewhat \\npoorer performance than other methods. Customization methods are further discussed in Part 3. \\nEvolution of Large Language Models  \\nAI systems were historically about processing and analyzing data, not generating it. They were more \\noriented toward perceiving and understanding the world around us rather than on generating new \\ninformation. This distinction marks the main difference between Perceptive and Generative AI, with \\nthe latter becoming increasingly prevalent since around 2020, or after companies started adopting \\ntransformer models and developing increasingly more robust LLMs at a large scale.  \\nThe advent of large language models further fueled a revolutionary paradigm shift in the way NLP \\nmodels are designed, trained, and used. To truly understand this, it may be helpful to compare large \\nlanguage models to previous NLP models and how they worked. For this purpose, let’s briefly explore \\nthree regimes in the history of NLP: pre-transformers NLP, transformers NLP, and LLM NLP. \\n1. Pre-transformers NLP was mainly marked by models that relied on human-crafted rules rather \\nthan machine learning algorithms to perform NLP tasks. This made them suitable for simpler tasks \\nthat didn’t require too many rules, like text classification, but unsuitable for more complex tasks, \\nsuch as machine translation. Rule-based models also performed poorly in edge-case scenarios \\nbecause they couldn’t make accurate predictions or classifications for never-before-seen data for \\nwhich no clear rules were set. This problem was somewhat solved with simple neural networks, \\nsuch as RNNs and LSTMs, developed during the later phases of this period. RNNs and LSTMs could \\nmemorize past data to a certain extent and, thus, provide context-dependent predictions and'), Document(metadata={'producer': 'macOS Version 12.6.1 (Build 21G217) Quartz PDFContext', 'creator': 'Microsoft Word', 'creationdate': '2023-03-17T18:08:34+00:00', 'author': 'NVIDIA', 'keywords': 'Large Language Models, What is a large language model, llm, how do large language models work', 'moddate': '2023-03-17T11:30:56-07:00', 'subject': 'Large Language Models', 'title': 'A Beginner’s Guide to Large Language Models', 'source': 'pdfs/nvidia_llm.pdf', 'total_pages': 25, 'page': 11, 'page_label': '12'}, page_content='A Beginner’s Guide to Large Language Models 12 \\n \\nclassifications. However, RNNs and LSTMs could not make predictions over long spans of text, \\nlimiting their effectiveness. \\n2. Transformers NLP was set in motion by the rise of the transformer architecture in 2017. \\nTransformers could generalize better than the then-prevailing RNNs and LSTMs,  capture more \\ncontext, and process more data at once. These improvements enabled NLP models to understand \\nlonger sequences of data and perform a much wider range of tasks. However, from today’s point \\nof view, models developed during this period had limited capabilities, mainly due to the general \\nlack of large-scale datasets and adequate computational resources. They also mainly sparked \\nattention among researchers and experts in the field but not the general public, as they weren’t \\nuser-friendly nor accurate enough to become commercialized. \\n3. LLM NLP was mainly initiated by the launch of OpenAI’s GPT-3 in 2020. Large language models \\nlike GPT-3 were trained on massive amounts of data, which allowed them to produce more \\naccurate and comprehensive NLP responses compared to previous models. This unlocked many \\nnew possibilities and brought us closer to achieving what many consider “true” AI. Also, LLMs \\nmade NLP models much more accessible to non-technical users who could now solve a variety of \\nNLP tasks just by using natural-language prompts. NLP technology was finally democratized. \\nThe switch from one methodology to another was largely driven by relevant technological and \\nmethodological advancements, such as the advent of neural networks, attention mechanisms, and \\ntransformers and developments in the field of unsupervised and self-supervised learning. The \\nfollowing sections will briefly explain these concepts, as understanding them is crucial for truly \\nunderstanding how LLMs work and how to build new LLMs from scratch.  \\nNeural Networks \\nNeural networks (NNs) are machine learning algorithms loosely modeled after the human brain. Like \\nthe biological human brain, artificial neural networks consist of neurons, also called nodes, that are \\nresponsible for all model functions, from processing input to generating output.  \\nThe neurons are further organized into layers, vertically stacked components of NNs that perform \\nspecific tasks related to input and output sequences.'), Document(metadata={'producer': 'macOS Version 12.6.1 (Build 21G217) Quartz PDFContext', 'creator': 'Microsoft Word', 'creationdate': '2023-03-17T18:08:34+00:00', 'author': 'NVIDIA', 'keywords': 'Large Language Models, What is a large language model, llm, how do large language models work', 'moddate': '2023-03-17T11:30:56-07:00', 'subject': 'Large Language Models', 'title': 'A Beginner’s Guide to Large Language Models', 'source': 'pdfs/nvidia_llm.pdf', 'total_pages': 25, 'page': 12, 'page_label': '13'}, page_content='A Beginner’s Guide to Large Language Models 13 \\n \\nEvery neural network has at least three layers: \\n> The input layer accepts data and passes it to the rest of the network.  \\n> The hidden layer, or multiple hidden layers, performs specific functions that make the final \\noutput of an NN possible. These functions can include identifying or classifying data, generating \\nnew data, and other functions depending on the specific NLP task in question. \\n> The output layer generates a prediction or classification based on the input. \\nWhen LLMs were first developed, they were based on simpler NN architectures with fewer layers, \\nmainly recurrent neural networks (RNNs) and long short-term memory networks (LSTMs). Unlike \\nother neural networks, RNNs and LSTMs could take into account the context, position, and \\nrelationships between words even if they were far apart in a data sequence. Simply put, this meant \\nthey could memorize and consider past data when generating output, which resulted in more \\naccurate solutions to many NLP tasks, especially sentiment analysis and text classification.  \\nThe biggest advantage that neural networks like RNNs and LSTMs had over traditional, rule-based \\nsystems was that they were capable of learning on their own with little to no human involvement. \\nThey analyze data to create their own rules, rather than learn the rules first and apply them to data \\nlater. This is also known as representation learning and is inspired by human learning processes.  \\nRepresentations, or features, are hidden patterns that neural networks can extract from data. To \\nexemplify this, let’s imagine we’re training an NN-based model on a dataset containing the following \\ntokens:  \\n“cat,” “cats,” dog,” “dogs”  \\nAfter analyzing these tokens, the model may identify a representation that one could formulate as: \\nPlural nouns have the suffix “-s.” \\nThe model will then extract this representation and apply it to new or edge-case scenarios whose data \\ndistribution follows that of training data. For example, the assumption can be made that the model \\nwill correctly classify tokens like “chairs” or “table” as plural or singular even if it had not encountered \\nthem before. Once it encounters irregular nouns that don’t follow the extracted representation, the \\nmodel will update its parameters to reflect new representations, such as:  \\nPlural nouns are followed by plural verbs. \\nThis approach enables NN-based models to generalize better than rule-based systems and \\nsuccessfully perform a wider range of tasks. \\nHowever, their ability to extract representations is very much dependent on the number of neurons \\nand layers comprising a network. The more neurons neural networks have, the more complex \\nrepresentations they can extract. That’s why, today, most large language models use deep learning \\nneural networks with multiple hidden layers and, thus, a higher number of neurons.'), Document(metadata={'producer': 'macOS Version 12.6.1 (Build 21G217) Quartz PDFContext', 'creator': 'Microsoft Word', 'creationdate': '2023-03-17T18:08:34+00:00', 'author': 'NVIDIA', 'keywords': 'Large Language Models, What is a large language model, llm, how do large language models work', 'moddate': '2023-03-17T11:30:56-07:00', 'subject': 'Large Language Models', 'title': 'A Beginner’s Guide to Large Language Models', 'source': 'pdfs/nvidia_llm.pdf', 'total_pages': 25, 'page': 13, 'page_label': '14'}, page_content='A Beginner’s Guide to Large Language Models 14 \\n \\nFigure 2 shows a side-by-side comparison of a single-layer neural network and a deep learning neural \\nnetwork.  \\nFigure 2. Comparison of Single-Layer vs. Deep Learning Neural Network \\n \\nWhile this may seem like an obvious choice today, consider that developing deep neural networks did \\nnot make sense before the hardware evolved to be able to handle massive workloads. This only \\nbecame possible after ~1999 when NVIDIA introduced “the world’s first GPU,” or graphics processing \\nunit, to the wider market or, more precisely, after a wildly successful CNN called AlexNet popularized \\ntheir use in deep learning in 2012. \\nGPUs had a highly parallelizable architecture which enabled the rapid advances in deep learning \\nsystems that are seen today. Among other advancements, the advent of GPUs ushered in the \\ndevelopment of a new type of neural network that would revolutionize the field of NLP: transformers.  \\nTransformers \\nWhile RNNs and LSTMs have their advantages, especially compared to traditional models, they also \\nhave some limitations that make them unsuitable for more complex NLP tasks, such as machine \\ntranslation. Their main limitation is the inability to process longer data sequences and, thus, consider \\nthe overall context of the input sequence. Because LSTMs and RNNs cannot handle too much context \\nwell, their outputs are prone to being inaccurate or nonsensical. This and other challenges have been \\nlargely overcome with the advent of new, special neural networks called transformers.  \\nTransformers were first introduced in 2017 by Vaswani et al. in a paper titled \"Attention is All You \\nNeed.\" The title alluded to attention mechanisms, which would become the key component of \\ntransformers.'), Document(metadata={'producer': 'macOS Version 12.6.1 (Build 21G217) Quartz PDFContext', 'creator': 'Microsoft Word', 'creationdate': '2023-03-17T18:08:34+00:00', 'author': 'NVIDIA', 'keywords': 'Large Language Models, What is a large language model, llm, how do large language models work', 'moddate': '2023-03-17T11:30:56-07:00', 'subject': 'Large Language Models', 'title': 'A Beginner’s Guide to Large Language Models', 'source': 'pdfs/nvidia_llm.pdf', 'total_pages': 25, 'page': 14, 'page_label': '15'}, page_content='A Beginner’s Guide to Large Language Models 15 \\n \\n“We propose a new simple network architecture, the Transformer, based solely on attention \\nmechanisms, dispensing with recurrence and convolutions entirely.” - Vaswani et. al, “Attention is All \\nYou Need”  \\nThe paper proposed that attention mechanisms would render recurrence and convolutions obsolete \\nin regard to sequential data and make transformers more suitable for machine translation than RNNs \\nand CNNs. This prophecy would soon come true, as transformers would go on to become the \\ndominant architecture not only for machine translation but for all NLP tasks.  \\nAttention mechanisms solved the problem of inadequate context handling by allowing models to \\nselectively pay attention to certain parts of the input while processing it. Instead of having to capture \\nthe entire context at once, models could now focus on the most important tokens regarding a specific \\ntask.  \\nTo demonstrate this, let’s imagine the desired model is a transformer-based model to predict the next \\nwords for the following input sentence:  \\nMary had a little lamb. \\nAttention mechanisms – or, rather, self-attention layers that are based on attention mechanisms – \\nwould first calculate attention weights for each word in our input. Attention weights represent the \\nimportance of each token, so the more weight a token is assigned, the more important it\\'s deemed. \\nFor example, the attention mechanism might give more weight to the word \"lamb\" than the word \"a,\" \\nas it’s likely to have more influence on the final output. \\nThe model would then use these weights to dynamically emphasize or downplay each word as it \\ngenerates output. If one assumes that the most weight was assigned to the word “lamb,” the model \\nmay produce a continuation such as: \\n\"whose fleece was white as snow\" \\nTo determine how important each token is, self-attention layers examine its relationships with other \\ntokens in a sequence:  \\n1. If a token has many relevant relationships with other tokens with respect to the task being \\nperformed, then that token is deemed as important and, potentially, more important than other \\ntokens in the same sequence. \\n2. If a token doesn’t have many relationships with other tokens, or if they are  irrelevant to a specific \\ntask, that token is considered less important or completely unimportant. This means the model \\nwill virtually ignore it when generating the output. \\nSo, by enabling models to handle context more effectively, attention mechanisms allowed them to \\ngenerate more accurate outputs than models based on RNNs and LSTMs. Simultaneously, this new \\napproach to data processing also allowed transformer-based models to generate outputs more \\nquickly than RNN- and LSTM-based models.'), Document(metadata={'producer': 'macOS Version 12.6.1 (Build 21G217) Quartz PDFContext', 'creator': 'Microsoft Word', 'creationdate': '2023-03-17T18:08:34+00:00', 'author': 'NVIDIA', 'keywords': 'Large Language Models, What is a large language model, llm, how do large language models work', 'moddate': '2023-03-17T11:30:56-07:00', 'subject': 'Large Language Models', 'title': 'A Beginner’s Guide to Large Language Models', 'source': 'pdfs/nvidia_llm.pdf', 'total_pages': 25, 'page': 15, 'page_label': '16'}, page_content='A Beginner’s Guide to Large Language Models 16 \\n \\nLSTMs and RNNs need more time to generate output because they process input sequentially. To \\nclarify what this means, let’s explore how LSTMs would approach processing our original input \\nsentence:  \\nMary had a little lamb. \\nSince LSTMs process data sequentially, they would need to process one word in our sequence at a \\ntime: Mary, had, a, little, lamb. This significantly slows down inference, especially with longer data \\nsequences. For example, just imagine how long it would take LSTMs and RNNs to process a single \\nWikipedia page. Too long.  \\nTransformers, on the other hand, process data in parallel, which means they “read” all input tokens at \\nonce instead of processing one at a time. It also means they are  able to perform NLP tasks faster than \\nLSTMs and RNNs.  \\nHowever, despite being slow, sequential data processing has one big advantage. By processing one \\nword at a time, LSTMs and RNNs are always able to tell which word came first, second, and so on. \\nThey know the word order of the input sequence because they use that same order to process it. \\nConversely, transformers are not initially “aware” of the original word order because they process \\ndata non-sequentially. While this may seem like only a minor problem at first, analyzing the sentences \\nbelow may illustrate otherwise:  \\n1. Mary had a little lamb.  \\n \\n2. A little lamb had Mary.  \\n \\n3. Had a little lamb Mary. \\nSentence (2) shows how a slight change in word order can distort the intended meaning, while \\nsentence (3) exemplifies an even bigger issue — how changes in word order can result in completely \\nnonsensical and grammatically incorrect variations. \\nTo overcome this challenge, transformers use positional encodings that help them retain position \\ninformation. Positional encodings are additional inputs, or vectors, associated with each token. They \\ncan be fixed or trainable, depending on whether the desire is for the model to refine them during \\ntraining or not.'), Document(metadata={'producer': 'macOS Version 12.6.1 (Build 21G217) Quartz PDFContext', 'creator': 'Microsoft Word', 'creationdate': '2023-03-17T18:08:34+00:00', 'author': 'NVIDIA', 'keywords': 'Large Language Models, What is a large language model, llm, how do large language models work', 'moddate': '2023-03-17T11:30:56-07:00', 'subject': 'Large Language Models', 'title': 'A Beginner’s Guide to Large Language Models', 'source': 'pdfs/nvidia_llm.pdf', 'total_pages': 25, 'page': 16, 'page_label': '17'}, page_content='A Beginner’s Guide to Large Language Models 17 \\n \\nFigure 3. Sequential Data Processing in a LSTM \\nSource: Attention Is All You Need \\n \\nResearchers and companies would soon start implementing these new mechanisms and building new \\ntransformer-based models, with Google releasing its famous BERT in 2018. \\nBERT \\nGoogle’s BERT (Bidirectional Encoder Representations from Transformers) is one of the first \\ntransformer-based language models. It’s a masked language model (MLM), which means it’s trained \\non sentences containing masked tokens. The model needs to predict the masked token by considering \\nits surrounding context. To illustrate this, let’s imagine that a model is given the following input \\nsentence: \\n\"I [have] a mask.\" \\nBERT’s task is to predict the masked word “have.” It does so by analyzing the tokens on both of its \\nsides, namely \"I,\", \"a,\" and “mask.” This is what makes it bidirectional, as well as more accurate than \\nprevious language models that could only consider the context on the left of the masked token. In this \\ncase, unidirectional models would only consider the word “I” when predicting the masked word, \\nwhich provides little context. The chances of a unidirectional model generating the right predictions \\nare smaller.'), Document(metadata={'producer': 'macOS Version 12.6.1 (Build 21G217) Quartz PDFContext', 'creator': 'Microsoft Word', 'creationdate': '2023-03-17T18:08:34+00:00', 'author': 'NVIDIA', 'keywords': 'Large Language Models, What is a large language model, llm, how do large language models work', 'moddate': '2023-03-17T11:30:56-07:00', 'subject': 'Large Language Models', 'title': 'A Beginner’s Guide to Large Language Models', 'source': 'pdfs/nvidia_llm.pdf', 'total_pages': 25, 'page': 17, 'page_label': '18'}, page_content=\"A Beginner’s Guide to Large Language Models 18 \\n \\nBERT was the first model to show how bidirectionality can model performance NLP tasks. It has been \\nused for various purposes, including improving the accuracy of Google's search results by allowing for \\na better understanding of the context and meaning of queries.  \\nOther Large Language Models \\nGoogle’s BERT and new advancements in the field inspired other companies to start building their \\nown large language models. In the table below, we’ve listed some models developed prior to the truly \\ngroundbreaking GPT-3, released in June 2020. \\nTable 2 shows a timeline of consequential LLM releases. \\nTable 2. Large Language Model Release Timeline \\nModel Company Year Brief Description \\nGPT-2 OpenAI 2019 A transformer-based language model designed to generate human-like \\ntext and perform various NLP tasks, including language translation, \\nsummarization, and question answering. \\nRoBERTa Facebook 2019 A BERT-based model designed to improve the performance of NLP tasks \\nby training the model on a larger dataset and using a more efficient \\ntraining method. \\nDeBERTa Microsoft 2020 A BERT-based model designed to improve the performance of NLP tasks \\nby decoupling the encoder and decoder components of the model. \\nGPT-3 OpenAI 2020 An upgraded model of GPT-2 trained on a more massive dataset and \\ncapable of generating higher-quality outputs. \\n \\nBy proving that LLMs can be used for few-shot learning and excel without “large-scale task-specific \\ndata collection or model parameter updating,” GPT-3 would inspire companies to build even larger \\nmodels, like Megatron-Turing Natural Language Generation with 530 billion parameters, PaLM with \\n540 billion, and WuDao 2.0 with impressive 1.75 trillion parameters. \\nUnsupervised and Self-Supervised Learning \\nBERT wasn’t revolutionary just because it was a bidirectional model, but also because it was trained \\nusing unsupervised learning. Unsupervised learning refers to machine learning algorithms finding \\npatterns in unlabeled datasets with no human intervention. In BERT’s case, the model had to extract \\npatterns from plain-language Wikipedia pages on its own during training. This is often considered to \\nbe AI in its purest form.  \\nUnsupervised learning models use feedback loops to learn and improve their performance. This \\ninvolves getting feedback on whether a prediction or classification was right or wrong, which the \\nmodel uses to guide its future decisions.  \\nFeedback loops are the reason why some differentiate between unsupervised and self-supervised \\nlearning. Self-supervised learning models don’t have feedback loops but rather use supervisory signals\"), Document(metadata={'producer': 'macOS Version 12.6.1 (Build 21G217) Quartz PDFContext', 'creator': 'Microsoft Word', 'creationdate': '2023-03-17T18:08:34+00:00', 'author': 'NVIDIA', 'keywords': 'Large Language Models, What is a large language model, llm, how do large language models work', 'moddate': '2023-03-17T11:30:56-07:00', 'subject': 'Large Language Models', 'title': 'A Beginner’s Guide to Large Language Models', 'source': 'pdfs/nvidia_llm.pdf', 'total_pages': 25, 'page': 18, 'page_label': '19'}, page_content='A Beginner’s Guide to Large Language Models 19 \\n \\nto get feedback during training. These signals are generated automatically from data without human \\nannotation.  \\nBoth unsupervised and self-supervised learning techniques have one key advantage over supervised \\nlearning: they rely on the model to create labels and extract features on its own, rather than \\ndemanding human intervention. This helps companies train models without time-consuming data \\nlabeling processes or providing human feedback on the model’s outputs. \\nSelf-supervised learning is currently the dominant approach to pre-training large language models and \\nis often recommended to enterprises that want to build their own.  \\nBenefits of GPT Vs Bert \\nGPT (Generative Pre-trained Transformer) and BERT (Bidirectional Encoder Representations from \\nTransformers) are both highly advanced and widely used natural language processing (NLP) models. \\nHowever, they differ in their architectures and use cases. \\nGPT is a generative model that is trained to predict the next word in a sentence given the previous \\nwords. This pre-training enables GPT to generate coherent and fluent sentences from scratch, making \\nit ideal for language generation tasks such as text completion, summarization, and question \\nanswering. \\nIn distinction, BERT is a discriminative model that is trained to classify sentences or tokens into \\ndifferent categories such as sentiment analysis, named entity recognition, and text classification. It is \\na bidirectional model that considers both the left and right contexts of a sentence to understand the \\nmeaning of a word, making it highly effective for tasks such as sentiment analysis and question \\nanswering. \\nIn terms of architecture, GPT uses a unidirectional transformer, whereas BERT uses a bidirectional \\ntransformer. This means that GPT can only consider the left context of a word when making \\npredictions, while BERT considers both the left and right contexts. \\nBoth GPT and BERT are powerful models that have revolutionized the field of NLP. Their choice \\ndepends on the specific task at hand, and researchers and practitioners often use a combination of \\nboth models to achieve optimal results.'), Document(metadata={'producer': 'macOS Version 12.6.1 (Build 21G217) Quartz PDFContext', 'creator': 'Microsoft Word', 'creationdate': '2023-03-17T18:08:34+00:00', 'author': 'NVIDIA', 'keywords': 'Large Language Models, What is a large language model, llm, how do large language models work', 'moddate': '2023-03-17T11:30:56-07:00', 'subject': 'Large Language Models', 'title': 'A Beginner’s Guide to Large Language Models', 'source': 'pdfs/nvidia_llm.pdf', 'total_pages': 25, 'page': 19, 'page_label': '20'}, page_content='A Beginner’s Guide to Large Language Models 20 \\n \\nHow Enterprises Can Benefit From Using \\nLarge Language Models \\nEnterprises need to tackle language-related tasks every day. This includes \\nmore obvious text tasks, such as writing emails or generating content, but also tasks like analyzing \\npatient data for health risks or providing companionship to customers. All of these tasks can be \\nautomated using large language models.  \\nModels, or applications powered by large language models, can help enterprises speed up many \\ncomplex tasks and often execute them with a higher level of precision than human agents. For \\nexample, tech enterprises can use them to write code faster, while banks can use them to minimize \\nthe risk of human error when analyzing documents for fraud indications. \\nAutomating complex, but often tedious tasks further allow employees to focus on more important \\ntasks instead and make progress faster. We’ll see, for example, how healthcare enterprises can use \\nLLMs to generate synthetic clinical data and use it to speed up medical research in Part 2. \\nLLMs can benefit enterprises in many other ways, depending on how they are  used. Some use cases, \\nlike LLM-based sentiment analysis, provide them with deeper insights about their audience, while \\ncustomer churn prediction allows them to encourage customers to stay with their company just as \\nthey were about to leave it. Additionally, enterprises can use LLMs to offer new, conversation-based \\nservices, such as specialized AI companions.'), Document(metadata={'producer': 'macOS Version 12.6.1 (Build 21G217) Quartz PDFContext', 'creator': 'Microsoft Word', 'creationdate': '2023-03-17T18:08:34+00:00', 'author': 'NVIDIA', 'keywords': 'Large Language Models, What is a large language model, llm, how do large language models work', 'moddate': '2023-03-17T11:30:56-07:00', 'subject': 'Large Language Models', 'title': 'A Beginner’s Guide to Large Language Models', 'source': 'pdfs/nvidia_llm.pdf', 'total_pages': 25, 'page': 20, 'page_label': '21'}, page_content='A Beginner’s Guide to Large Language Models 21 \\n \\nChallenges of Large Language Models \\nEnterprises that want to start using large language models, or applications powered by large language \\nmodels, should be aware of a few common LLM-related pitfalls. Below are some general ones that are \\napplicable regardless of whether a model is being customized, fine-tuned, or built from scratch. \\n1. Large language models are vulnerable to adversarial examples. Adversarial examples are \\ninputs specifically crafted to fool the models into making a mistake. This can raise security \\nconcerns, particularly for enterprises in sensitive industries like healthcare or finance. \\n2. Large language models can lack interpretability. Interpretability refers to the ability to \\ninterpret and predict models’ decisions. Models with low interpretability can be difficult to \\ntroubleshoot and evaluate, as it may not be clear how they are  making their decisions or how \\naccurate or unbiased those decisions are. This can be especially problematic in the context of \\nhigh-stake use cases, such as fraud detection, and in industries that require a high level of \\ntransparency, such as healthcare and finance.  \\n3. Large language models may provide un-customized, generic answers. As such, LLMs may not \\nalways respond well to human input or understand the intent behind it. This can be improved \\nwith techniques such as Reinforcement Learning from Human Feedback (RLHF), which help \\nmodels improve their performance over time based on positive or negative human feedback. \\nEven so, LLMs can sometimes reproduce the text data they’ve seen during training. This is \\nproblematic only from an ethical angle but may also expose enterprises to unwanted \\ncopyright and legal issues.  \\n4. Using large language models can raise ethical concerns. It’s questionable whether enterprises \\nshould use LLMs for important decision-making tasks, such as deciding which candidate is the \\nmost qualified based on collected resumes, especially without human supervision. \\nAdditionally, it should be assessed whether it’s ethical to use LLMs for tasks that would \\nnormally be performed by human, mainly white-collar workers.   \\n5. Large language models can generate inappropriate and harmful content. On that note, \\nenterprises should keep in mind that LLMs are often trained on large corpora of Internet \\ntexts, which may make them prone to generating toxic, biased, and otherwise inappropriate \\nand harmful content. \\nEnterprises that want to build proprietary LLMs from scratch also need to address additional \\nchallenges, like whether they have sufficient computing power, storage, and datasets, expertise, and \\nfinancial resources to develop, implement, and maintain the models.  \\nWays to Build LLMs  \\nBuilding large language models from scratch doesn’t always make sense, especially for enterprises \\nwhose core business is not related to AI or NLP technologies. Since the process can be extremely \\ntime-consuming and resource-exhaustive, most enterprises are more likely to opt for customizing \\nexisting models to their needs.'), Document(metadata={'producer': 'macOS Version 12.6.1 (Build 21G217) Quartz PDFContext', 'creator': 'Microsoft Word', 'creationdate': '2023-03-17T18:08:34+00:00', 'author': 'NVIDIA', 'keywords': 'Large Language Models, What is a large language model, llm, how do large language models work', 'moddate': '2023-03-17T11:30:56-07:00', 'subject': 'Large Language Models', 'title': 'A Beginner’s Guide to Large Language Models', 'source': 'pdfs/nvidia_llm.pdf', 'total_pages': 25, 'page': 21, 'page_label': '22'}, page_content='A Beginner’s Guide to Large Language Models 22 \\n \\nCustomizing existing base models – also called pre-trained models or PLMs –  can be typically split \\ninto three essential steps: \\n1. Finding a well-suited foundation model (PLM). This requires considering ideal model size, \\ntraining tasks and datasets, LLM providers, and more.  \\n2. Fine-tuning the model. Base models can be fine-tuned on a specific corpus and for a specific use \\ncase. For example, text classification base models may be fine-tuned for sentiment analysis or \\ntrained using legal records to become proficient in legal terminology.  \\n3. Optimizing the model. Models can be further optimized using techniques such as Reinforcement \\nLearning from Human Feedback (RLHF), where the model is updated based on positive or \\nnegative human feedback on its predictions or classifications. RLHF seems particularly promising, \\npartially due to being used in widely popular ChatGPT.  \\nAlternatively, enterprises may choose to only customize base models using parameter-efficient \\ntechniques like adapters and p-tuning. Customization can yield especially accurate models when the \\nbase model is trained on tasks similar to the selected downstream tasks. For example, a base text \\nclassification model may be a good candidate for customization for sentiment analysis, as the two \\ntasks are very similar. Thanks to being trained on text classification, the model can draw upon the \\nknowledge it gained during training to perform sentiment analysis tasks more easily. \\nHow to Evaluate LLMs \\nLarge language models (LLMs) use deep learning techniques to analyze and generate natural \\nlanguage. These models have become increasingly popular due to their ability to perform a wide \\nrange of language-related tasks such as language translation, text summarization, and question-\\nanswering. However, evaluating the performance of LLMs is not a straightforward task, and it requires \\na careful analysis of different factors such as training data, model size, and speed of inference. \\nThe most crucial element in evaluating LLMs is the quality and quantity of the training data used. The \\ntraining data should be diverse and representative of the target language and domain to ensure that \\nthe LLM can learn and generalize language patterns effectively. Moreover, the training data should be \\nannotated with relevant labels or tags to enable supervised learning, which is the most used approach \\nin LLMs. \\nAnother important factor is the size of the model. Generally, larger models have better performance, \\nbut they also require more computational resources to train and run. Therefore, researchers often \\nuse a trade-off between model size and performance, depending on the specific task and resources \\navailable. It is also worth noting that larger models tend to be more prone to overfitting, which can \\nlead to poor generalization performance on new data. \\nSpeed of inference must also be used in evaluation, especially when deploying LLMs in real-world \\napplications. Faster inference time is desirable as it enables the LLM to process large amounts of data \\nin a timely and efficient manner. Several techniques, such as pruning, quantization, and distillation, \\nhave been proposed to reduce the size and improve the speed of LLMs.'), Document(metadata={'producer': 'macOS Version 12.6.1 (Build 21G217) Quartz PDFContext', 'creator': 'Microsoft Word', 'creationdate': '2023-03-17T18:08:34+00:00', 'author': 'NVIDIA', 'keywords': 'Large Language Models, What is a large language model, llm, how do large language models work', 'moddate': '2023-03-17T11:30:56-07:00', 'subject': 'Large Language Models', 'title': 'A Beginner’s Guide to Large Language Models', 'source': 'pdfs/nvidia_llm.pdf', 'total_pages': 25, 'page': 22, 'page_label': '23'}, page_content='A Beginner’s Guide to Large Language Models 23 \\n \\nTo evaluate the performance of LLMs, researchers often use benchmarks, which are standardized \\ndatasets and evaluation metrics for a particular language-related task. Benchmarks enable fair \\ncomparisons between different models and methods and help identify the strengths and weaknesses \\nof LLMs. Common benchmarks include GLUE (General Language Understanding Evaluation), \\nSuperGLUE, and CoQA (Conversational Question Answering). \\nNotable Companies in the LLM Field \\nThe release of BERT in 2018 and, more notably, the release of GPT-3 in 2020, prompted both large \\ntech companies and smaller startups to enter the race with their own LLMs and innovative \\napproaches to model development. The most notable companies developing their own LLMs at the \\ntime of publication are listed in Table 3. \\nTable 3. Notable Companies Developing LLMs \\nCompany LLM \\nOpenAI GPT-3 Davinci (175B) \\nAI21 Labs Jurassic-1-Jumbo (178B) \\nEleutherAI  GPT-NeoX (20B) \\nAnthropic  Anthropic-LM (52B) \\nCohere Cohere xlarge v20220609 (52.4B) \\nNVIDIA/Microsoft Megatron-Turing Natural Language Generation (MT-NLG 530B) \\nMicrosoft Turing Natural Language Generation (T-NLG 17B) \\nGoogle Pathways Language Model (PaLM 540B) \\nMeta Open Pretrained Transformer (OPT-175B) \\n \\nSome of these companies offer other organizations access to their models. For example, enterprises \\ncan customize pre-trained modes developed by OpenAI, Cohere, or NVIDIA for downstream tasks or \\nintegrate them into their products and internal systems via API. \\nPopular Startup-developed LLM Apps \\nOpenAI’s ChatGPT is by far the most popular LLM-powered app developed to date. It’s estimated that \\nit attracted over 100 million users in just two months after its launch, making it the “fastest-growing \\nconsumer application in history.”  \\nHowever, many other startups entered the ring with their own, often more specialized and \\ncommercialized LLM-powered apps. One of the most popular such apps are LLM-powered content \\ngenerators like Jasper and Copy.ai. For comparison, Jasper boasts catering to over 100,000 global \\nteams, while Copy.ai claims it has attracted over 5,000,000 users since its launch.  \\nFigure 4 shows an example of a natural-language prompt that users can enter in to Copy.ai to \\ngenerate a blog post outline.'), Document(metadata={'producer': 'macOS Version 12.6.1 (Build 21G217) Quartz PDFContext', 'creator': 'Microsoft Word', 'creationdate': '2023-03-17T18:08:34+00:00', 'author': 'NVIDIA', 'keywords': 'Large Language Models, What is a large language model, llm, how do large language models work', 'moddate': '2023-03-17T11:30:56-07:00', 'subject': 'Large Language Models', 'title': 'A Beginner’s Guide to Large Language Models', 'source': 'pdfs/nvidia_llm.pdf', 'total_pages': 25, 'page': 23, 'page_label': '24'}, page_content='A Beginner’s Guide to Large Language Models 24 \\n \\nFigure 4. Example of Results of a Natural Language Prompt \\n \\nOther examples of popular LLM-powered apps include the beloved grammar-checking and writing \\ntool, Grammarly, and GitHub Copilot, a Codex-powered coding assistant that can help developers \\nwrite and learn code. \\nPart 2 will cover more ways in which enterprises and startups can leverage LLMs to build specialized \\napps for content generation, anomaly detection, toxicity classification, and other advanced NLP use \\ncases. It will also provide concrete examples of how they can be further customized to answer the \\nneeds of various industries, such as finance, healthcare, and telecommunications, in hopes of \\ninspiring organizations to use LLMs to unlock new possibilities in their respective industries.'), Document(metadata={'producer': 'macOS Version 12.6.1 (Build 21G217) Quartz PDFContext', 'creator': 'Microsoft Word', 'creationdate': '2023-03-17T18:08:34+00:00', 'author': 'NVIDIA', 'keywords': 'Large Language Models, What is a large language model, llm, how do large language models work', 'moddate': '2023-03-17T11:30:56-07:00', 'subject': 'Large Language Models', 'title': 'A Beginner’s Guide to Large Language Models', 'source': 'pdfs/nvidia_llm.pdf', 'total_pages': 25, 'page': 24, 'page_label': '25'}, page_content='NVIDIA Corporation  |  2788 San Tomas Expressway, Santa Clara, CA 95051 \\nhttp://www.nvidia.com \\nNotice \\nThis document is provided for information p urposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA \\nCorporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and \\nassumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such  information or for any infringement of patents \\nor other rights of third parties that may res ult from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or \\nfunctionality. \\nNVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. \\nCustomer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. \\nNVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual \\nsales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly object s to applying any customer gener al terms \\nand conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are  formed either directly or indirectly by this \\ndocument. \\nNVIDIA products are not designed, authorized, or warranted to be s uitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where \\nfailure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability \\nfor inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. \\nNVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not \\nnecessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any info rmation contained in this document, ensure the \\nproduct is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the \\nproduct. Weaknesses in customer’s product designs may affect the quality and re liability of the NVIDIA product and may result in additional or different conditions and/or \\nrequirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable \\nto: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. \\nNo license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual pr operty right under this document. Information \\npublished by NVIDIA regarding third -party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endors ement \\nthereof. Use of such information may require a l icense from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDI A \\nunder the patents or other intellectual property rights of NVIDIA. \\nReproduction of information in this document is permissible on ly if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all \\napplicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. \\nTHIS DOCUMENT AND ALL NVIDIA DESIG N SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND \\nSEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE \\nMATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT \\nNOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT , INDIRECT, SPECIAL, INCIDENTAL, \\nPUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF \\nNVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate \\nand cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. \\nTrademarks \\nNVIDIA, the NVIDIA logo are trademarks and/or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be \\ntrademarks of the respective companies with which they are associated. \\nCopyright  \\n© 2023 NVIDIA Corporation. All rights reserved.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Split Extracted text in Chunks**"
      ],
      "metadata": {
        "id": "hUMIGfn-lPwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=20,\n",
        "    length_function=len\n",
        ")"
      ],
      "metadata": {
        "id": "6adoigO0lSjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the document into chunks\n",
        "chunks = text_splitter.split_documents(text_data)"
      ],
      "metadata": {
        "id": "8cIZINHdld9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "def wrap_text(text, width):\n",
        "  return textwrap.fill(text, width=width)\n",
        "\n",
        "for index in range(4):\n",
        "  print(f\"Chunk_{index+1}\\n\\n{wrap_text(chunks[index].page_content, 80)}\")\n",
        "  print(\"\\n================================================================\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-nlZdvZboqDD",
        "outputId": "ecb55954-57eb-4225-b03d-cb253543f2d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk_1\n",
            "\n",
            "A Beginner’s Guide to  Large Language Models Part 1 Contributors: Annamalai\n",
            "Chockalingam Ankur Patel Shashank Verma Tiffany Yeung\n",
            "\n",
            "================================================================\n",
            "\n",
            "Chunk_2\n",
            "\n",
            "A Beginner’s Guide to Large Language Models  2  Table of Contents  Preface .....\n",
            "................................................................................\n",
            ".................................................................. 3  Glossary .\n",
            "................................................................................\n",
            "..................................................................... 5\n",
            "\n",
            "================================================================\n",
            "\n",
            "Chunk_3\n",
            "\n",
            "Introduction to LLMs............................................................\n",
            "...................................................................... 8  What\n",
            "Are Large Language Models (LLMs)? ..............................................\n",
            "............................................ 8  Foundation Language Models vs.\n",
            "Fine-Tuned Language Models\n",
            "...................................................... 11\n",
            "\n",
            "================================================================\n",
            "\n",
            "Chunk_4\n",
            "\n",
            "Evolution of Large Language Models .............................................\n",
            ".................................................... 11  Neural Networks .......\n",
            "................................................................................\n",
            "...................................... 12  Transformers ........................\n",
            "................................................................................\n",
            ".......................... 14\n",
            "\n",
            "================================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZnIrdFUlnjr",
        "outputId": "0e1cdfba-f886-4a1d-cfbb-1129b38e963d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "128"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Downloading the Embeddings**"
      ],
      "metadata": {
        "id": "FCP60uZEpOJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "# include your openai api key in the secrets section\n",
        "# of this notebook and turn the notebook access on.\n",
        "from google.colab import userdata\n",
        "openai.api_key = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "XFVMBnp_pRXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key)"
      ],
      "metadata": {
        "id": "5mC-gVCNqLAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = embeddings.embed_query(\"Hello, world!\")"
      ],
      "metadata": {
        "id": "TD4DA4YWqK95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBkxpHn7qK7T",
        "outputId": "351a91a0-c2b2-484c-c3c2-036df10ad02d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1536"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Initializing Pinecone**"
      ],
      "metadata": {
        "id": "qaVKEOeKrP-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# API key only (no environment needed for new serverless)\n",
        "PINECONE_API_KEY = userdata.get('PINECONE_API_KEY')"
      ],
      "metadata": {
        "id": "IHeMlX1rqK41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pinecone import Pinecone\n",
        "\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "\n",
        "index_name = \"pinecone-1536-cosine\""
      ],
      "metadata": {
        "id": "xxnWxZtrqK2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Create Embeddings for each Chunk**"
      ],
      "metadata": {
        "id": "vtUD3yAKtb3r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "# Create docsearch (vector store)\n",
        "vectordb = PineconeVectorStore.from_texts(\n",
        "    [t.page_content for t in chunks],\n",
        "    embedding=embeddings,\n",
        "    index_name=index_name\n",
        ")"
      ],
      "metadata": {
        "id": "gxrpGsLFqKud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **If you already have an index, you can load it like this**"
      ],
      "metadata": {
        "id": "QBgBbSCi4-8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectordb = PineconeVectorStore.from_existing_index(\n",
        "    index_name=index_name,\n",
        "    embedding=embeddings\n",
        ")"
      ],
      "metadata": {
        "id": "p9tym-zJwq1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Similarity Search**"
      ],
      "metadata": {
        "id": "y5rHelVO5r3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query1 = \"What is an Large Language Model?\"\n",
        "query2 = \"Why are Large Language Models useful?\""
      ],
      "metadata": {
        "id": "xEicv4BN5uA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# k= how many similar_docs to return\n",
        "similar_docs = vectordb.similarity_search(query2, k=3)"
      ],
      "metadata": {
        "id": "M67jmRnM5z2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index in range(len(similar_docs)):\n",
        "  print(f\"doc_{index+1}\\n\\n{wrap_text(similar_docs[index].page_content, 80)}\")\n",
        "  print(\"\\n================================================================\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NYFphg0u6Aaw",
        "outputId": "62503c26-1545-4779-c4ad-96d7e7e890b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "doc_1\n",
            "\n",
            "A Beginner’s Guide to Large Language Models 20    How Enterprises Can Benefit\n",
            "From Using  Large Language Models  Enterprises need to tackle language-related\n",
            "tasks every day. This includes  more obvious text tasks, such as writing emails\n",
            "or generating content, but also tasks like analyzing  patient data for health\n",
            "risks or providing companionship to customers. All of these tasks can be\n",
            "automated using large language models.\n",
            "\n",
            "================================================================\n",
            "\n",
            "doc_2\n",
            "\n",
            "A Beginner’s Guide to Large Language Models 20    How Enterprises Can Benefit\n",
            "From Using  Large Language Models  Enterprises need to tackle language-related\n",
            "tasks every day. This includes  more obvious text tasks, such as writing emails\n",
            "or generating content, but also tasks like analyzing  patient data for health\n",
            "risks or providing companionship to customers. All of these tasks can be\n",
            "automated using large language models.\n",
            "\n",
            "================================================================\n",
            "\n",
            "doc_3\n",
            "\n",
            "A Beginner’s Guide to Large Language Models 9    Although all language models\n",
            "can perform NLP tasks, they differ in other characteristics, such as their\n",
            "size. Unlike other models, LLMs are considered large in size because of two\n",
            "reasons:   1. They’re trained using large amounts of data.  2. They comprise a\n",
            "huge number of learnable parameters (i.e., representations of the underlying\n",
            "structure of training data that help models perform tasks on new or never-\n",
            "before-seen data).\n",
            "\n",
            "================================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Creating LLM Wrapper for Structured Answer**"
      ],
      "metadata": {
        "id": "wIojCxCS7TUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAI\n",
        "\n",
        "llm = OpenAI(temperature=0, openai_api_key=openai.api_key)"
      ],
      "metadata": {
        "id": "a9cCGIJt6AWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vectordb.as_retriever()\n",
        ")"
      ],
      "metadata": {
        "id": "PWkgNswq6ATz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question/Answering**"
      ],
      "metadata": {
        "id": "sVnVHY8t-tus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qa.invoke(query1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COCNAXoN6ARd",
        "outputId": "7357520a-202b-43f3-eb94-cbadaf71cbe7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'What is an Large Language Model?',\n",
              " 'result': ' A large language model is a type of artificial intelligence system that is capable of generating human-like text based on the patterns and relationships it learns from vast amounts of data. It uses deep learning to analyze and process large sets of data, such as books, articles, and web pages.'}"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa.invoke(query2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nlbre1wa6AOy",
        "outputId": "cb42e6a2-867b-4a81-cc22-ff01f3a06eae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'Why are Large Language Models useful?',\n",
              " 'result': ' Large Language Models are useful because they can automate various language-related tasks, such as writing emails, generating content, analyzing data, and providing customer companionship. They are considered large in size because they are trained using large amounts of data and have a huge number of learnable parameters, making them more accurate and efficient in performing tasks on new or never-before-seen data.'}"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Interactive Loop for QA**"
      ],
      "metadata": {
        "id": "ycEUmn5aB7qh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "def chat_loop(qa_chain):\n",
        "    print(\"🤖 Chatbot ready! Type 'exit' to quit.\\n\")\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            user_input = input(\"You: \").strip()\n",
        "\n",
        "            if user_input.lower() in {\"exit\", \"quit\"}:\n",
        "                print(\"👋 Exiting chatbot. Bye!\")\n",
        "                break\n",
        "\n",
        "            if not user_input:\n",
        "                continue  # skip empty inputs\n",
        "\n",
        "            result = qa_chain({\"query\": user_input})\n",
        "            answer = result.get(\"result\", \"⚠️ Sorry, I couldn't generate an answer.\")\n",
        "\n",
        "            print(f\"Bot: {answer}\\n\")\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n🛑 Interrupted. Exiting chatbot.\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Error: {e}\\n\")\n",
        "\n",
        "# Usage\n",
        "chat_loop(qa)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yf4zXn1MB7bn",
        "outputId": "1056468d-3c62-4d33-c65f-977c0868e4da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🤖 Chatbot ready! Type 'exit' to quit.\n",
            "\n",
            "You: What is the use of an LLM?\n",
            "Bot:  The use of an LLM is to unlock cutting-edge possibilities and revolutionize operations for enterprises.\n",
            "\n",
            "You: Who developed the LLM?\n",
            "Bot:  The LLM was developed by various companies and startups in the LLM field.\n",
            "\n",
            "You: \n",
            "You: exit\n",
            "👋 Exiting chatbot. Bye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6jXcCo3QB7Y9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FTdKZcBIB7B5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}